{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['id', 'date', 'username', 'text', 'type', 'replies', 'favs', 'messages', 'followers', 'friends', 'lists']\n",
    "pos_db = pd.read_csv('positive.csv', delimiter=';', names=col_names).sample(1000)['username']\n",
    "neg_db = pd.read_csv('negative.csv', delimiter=';', names=col_names).sample(1000)['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.concat([pos_db, neg_db], ignore_index=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['снова в кино.\\nфильм оказался уж точно не моим.\\nхорошо что есть книга)',\n",
       "       '@_8763731544813 о! пугач! значит я не молодежь? (((',\n",
       "       'братан в парочке♥\\nзаюша Веруша:* http://t.co/qwTgHq6R3I', ...,\n",
       "       '\"@taniastern: у дженнифер эннистон оказывается было 2 выкидыша, а у кортни кокс целых 8\" ОБОЖЕ!!! Ужасссс(',\n",
       "       'Вот откуда любовь к совам....\\nИли к Франции))) http://t.co/qW1Flq3cUj',\n",
       "       'RT @domanep: Классно будет,если создатели Subway Surf придумали новую тему в Хогвартсе:пробегать сквозь стены,собирать волшебные палочки...)'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "def lemmatize(sent):\n",
    "    to_lemmatize = []\n",
    "    words = re.findall('\\w+', sent)\n",
    "    for word in words:\n",
    "        if not word.startswith('@') or not word.startswith('http'):\n",
    "            if word != 'RT':\n",
    "                if word.isalpha():\n",
    "                    if re.findall('[a-zA-Z]', word)==[]:\n",
    "                        to_lemmatize.append(word)\n",
    "    lemmas = []\n",
    "    #processed_sent = ' '.join(to_lemmatize)\n",
    "    for l in to_lemmatize:\n",
    "        try:\n",
    "            lemma = m.analyze(l)[0]['analysis'][0]['lex'] + '_' + m.analyze(l)[0]['analysis'][0]['gr'].split(',')[0]\n",
    "            if '=' in lemma:\n",
    "                lemma = lemma[:lemma.find('=')]\n",
    "            lemmas.append(lemma)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    processed_sent = ' '.join(lemmas)\n",
    "    return lemmas, processed_sent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for twit in texts[:500]:\n",
    "    lms, lm_sent = lemmatize(twit)\n",
    "    with open('lemmas.txt', 'a', encoding='utf8') as lemmas_file:\n",
    "        lemmas_file.write('\\n'.join(lms))\n",
    "    with open('twits.txt', 'a', encoding='utf8') as twit_file:\n",
    "        twit_file.write(lm_sent)\n",
    "        twit_file.write('\\n*****\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем список лемм (расклеиваем склеившиеся строки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ненавидеть_Vэто_SPRO\n",
    "ls = open('lemmas.txt', 'r', encoding='utf8').readlines()\n",
    "patt = re.compile('[A-Z][а-я]')\n",
    "lemmas = set()\n",
    "for l in ls:\n",
    "    if re.findall(patt, l) != []:\n",
    "        for i,el in enumerate(l.split('_')[1]):\n",
    "            if el.lower() == el:\n",
    "                idx = i + len(l.split('_')[0]) + 1\n",
    "                break\n",
    "        lemmas.add(l[:idx])\n",
    "        lemmas.add(l[idx:][:-1])\n",
    "    else:\n",
    "        lemmas.add(l[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = open('twits.txt', 'r', encoding='utf8').read().split('\\n*****\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('ruscorpora_mystem_cbow_300_2_2015.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = {}\n",
    "for lemma in list(lemmas):\n",
    "    try:\n",
    "        word_vecs[lemma] = model[lemma]\n",
    "    except KeyError:\n",
    "        pass\n",
    "with open('dict.txt', 'w', encoding='utf8') as d:\n",
    "    for word in word_vecs:\n",
    "        to_write = word + ' : ' + str(word_vecs[word])\n",
    "        d.write(to_write)\n",
    "        d.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = []\n",
    "for doc in twits:\n",
    "    matr = []\n",
    "    for word in doc.split(' '):\n",
    "        if word in word_vecs.keys():\n",
    "            matr.append(list(word_vecs[word]))\n",
    "    doc_vec = []\n",
    "    for i in range(0,300):\n",
    "        try:\n",
    "            doc_vec.append(max([line[i] for line in matr]))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    doc_vecs.append(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_vecs.txt', 'w', encoding='utf8') as dv:\n",
    "    for vec in doc_vecs:\n",
    "        dv.write(str(vec))\n",
    "        dv.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaleshin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftexts = []\n",
    "for twit in twits:\n",
    "    ftexts.append([el.split('_')[0] for el in twit.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText(ftexts, size=200, min_count=1, iter=40, word_ngrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aaleshin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "ft_dict = {}\n",
    "for lemma in lemmas:\n",
    "    ft_dict[lemma.split('_')[0]] = model_ft[lemma.split('_')[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1907"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
